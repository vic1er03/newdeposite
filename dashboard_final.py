"""Tableau de bord interactif pour l'analyse des donn√©es de donneurs de sang.
Ce script cr√©e un tableau de bord Streamlit avec des visualisations innovantes
pour r√©pondre aux objectifs du concours de data visualisation.
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import folium
from folium.plugins import MarkerCluster, HeatMap
from streamlit_folium import folium_static
import geopandas as gpd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import joblib
import nltk
from geopy.geocoders import Nominatim
from geopy.extra.rate_limiter import RateLimiter
from tqdm import tqdm
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from textblob import TextBlob
from wordcloud import WordCloud
from sklearn.metrics import roc_curve, auc, classification_report,confusion_matrix
import seaborn as sns
from sklearn.preprocessing import label_binarize
from folium.plugins import MarkerCluster
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score
from sklearn.metrics import accuracy_score, f1_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
import numpy as np
# Cr√©er un pipeline pour le pr√©traitement et la mod√©lisation
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
import os
import base64
import warnings
import io
import json
warnings.filterwarnings('ignore')

def paginate_dataframe(dataframe, page_size=10):
    """Ajoute une pagination √† un DataFrame pour am√©liorer les performances."""
    # Obtenir le nombre total de pages
    n_pages = len(dataframe) // page_size + (1 if len(dataframe) % page_size > 0 else 0)
    
    # Ajouter un s√©lecteur de page
    page = st.selectbox('Page', range(1, n_pages + 1), 1)
    
    # Afficher la page s√©lectionn√©e
    start_idx = (page - 1) * page_size
    end_idx = min(start_idx + page_size, len(dataframe))
    
    return dataframe.iloc[start_idx:end_idx]


# Configuration de la page Streamlit
st.set_page_config(
    page_title="Analyse des Donneurs de Sang",
    page_icon="üíâ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# D√©finir les chemins des fichiers
data_2019_path = "data_2019_preprocessed.csv"
data_volontaire_path = "data_volontaire_preprocessed.csv"
df_2020 = pd.read_csv("data_2020_pretraite.csv")
df_volontaire = pd.read_csv(data_volontaire_path)
df=df_volontaire.copy()
df_volontaires=df_volontaire.copy()
#analysis_results_dir = r"C:\Users\Ultra Tech\Desktop\analysis_results"
#model_path = r"C:\Users\Ultra Tech\Desktop\preprocessor_eligibility.pkl"

# Fonction pour charger les donn√©es
@st.cache_data
@st.cache_data(ttl=3600, max_entries=2)


def load_data():
    """
    #Charge les donn√©es pr√©trait√©es √† partir des fichiers CSV.
    """
    df_2019 = pd.read_csv(data_2019_path)
    df_volontaire = pd.read_csv(data_volontaire_path)
    
    # Convertir les colonnes de dates au format datetime
    date_columns = [col for col in df_2019.columns if 'date' in col.lower()]
    for col in date_columns:
        if col in df_2019.columns:
            try:
                df_2019[col] = pd.to_datetime(df_2019[col], errors='coerce')
            except:
                pass
    
    date_columns = [col for col in df_volontaire.columns if 'date' in col.lower()]
    for col in date_columns:
        if col in df_volontaire.columns:
            try:
                df_volontaire[col] = pd.to_datetime(df_volontaire[col], errors='coerce')
            except:
                pass
    
    return df_2019, df_volontaire

@st.cache_data

# Fonction pour charger le cache de g√©ocodage
def load_geo_cache():
    cache_file = "geo_cache.json"
    if os.path.exists(cache_file):
        with open(cache_file, "r") as f:
            return json.load(f)
    return {}

# Fonction pour sauvegarder le cache de g√©ocodage
def save_geo_cache(geo_cache):
    cache_file = "geo_cache.json"
    with open(cache_file, "w") as f:
        json.dump(geo_cache, f)

# Fonction pour obtenir les coordonn√©es avec cache
def get_coordinates_with_cache(row, geo_cache, geolocator, geocode):
    modalites_indesirables = [
        "Pas precise", "Pas pr√©cis√©", "Pas precise", "Pas pr√©cise", "Non pr√©cis√©", "non pr√©cis√©",
        " RAS", "Ras", "R A S ", "ras", "r a s", " ", "nan",
        " Pas mentionn√©", "Pasmentionn√©", "Pas mentionne"
    ]
    
    # V√©rifier si les colonnes n√©cessaires existent
    quartier = row.get('Quartier', '')
    arrondissement = row.get('Arrondissement', '')
    
    # Construire la requ√™te
    query = f"{quartier}, {arrondissement}, Douala, Cameroun"
    
    # V√©rifier si l'adresse est d√©j√† dans le cache
    if query in geo_cache:
        return geo_cache[query]['latitude'], geo_cache[query]['longitude']
    
    # V√©rifier si les donn√©es sont valides avant d'envoyer la requ√™te
    if pd.isna(quartier) or pd.isna(arrondissement) or quartier in modalites_indesirables:
        geo_cache[query] = {'latitude': 4.0483, 'longitude': 9.7043}  # Coordonn√©es par d√©faut de Douala
        return 4.0483, 9.7043
    
    try:
        location = geocode(query)
        if location:
            geo_cache[query] = {'latitude': location.latitude, 'longitude': location.longitude}
            return location.latitude, location.longitude
        else:
            geo_cache[query] = {'latitude': 4.0483, 'longitude': 9.7043}  # Valeur par d√©faut
            return 4.0483, 9.7043
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Erreur pour {query}: {e}")
        geo_cache[query] = {'latitude': 4.0483, 'longitude': 9.7043}  # Valeur par d√©faut
        return 4.0483, 9.7043

# Fonction pour cr√©er la carte Folium
def create_map(df):
    # Cr√©er une carte centr√©e sur Douala
    m = folium.Map(location=[4.0483, 9.7043], zoom_start=12, tiles='CartoDB positron')
    
    # Cr√©er un cluster de marqueurs pour am√©liorer les performances
    marker_cluster = MarkerCluster().add_to(m)
    
    # Ajouter des marqueurs pour chaque donneur
    for idx, row in df.iterrows():
        if pd.isna(row['latitude']) or pd.isna(row['longitude']):
            continue
            
        # Pr√©parer le contenu du popup
        popup_content = f"""
        <b>{row.get('Genre', 'N/A')}, {row.get('Age', 'N/A')} ans</b><br>
        <i>{row.get('Profession', 'N/A')}</i><br>
        √âligible: {row.get('Eligibilite_Don', 'N/A')}<br>
        Quartier: {row.get('Quartier', 'N/A')}<br>
        Arrondissement: {row.get('Arrondissement', 'N/A')}
        """
        
        # D√©terminer la couleur du marqueur en fonction de l'√©ligibilit√©
        eligibilite = str(row.get('Eligibilite_Don', '')).lower()
        color = 'green' if eligibilite == 'oui' else 'red' if eligibilite == 'non' else 'blue'
        
        # Ajouter le marqueur au cluster
        folium.Marker(
            location=[row['latitude'], row['longitude']],
            popup=folium.Popup(popup_content, max_width=250),
            icon=folium.Icon(color=color, icon='tint', prefix='fa')
        ).add_to(marker_cluster)
    
    return m

# Fonction pour cr√©er une carte de distribution g√©ographique
@st.cache_data
def create_geo_map(df, location_column, color_column=None, zoom_start=10):
    """
    Cr√©e une carte interactive montrant la distribution g√©ographique des donneurs.
    Utilise les coordonn√©es charg√©es depuis le fichier JSON.
    """
    # Charger les coordonn√©es g√©ographiques
    geo_data = load_geo_cache()
    
    # D√©terminer le type de localisation (arrondissement, quartier, ville)
    if "Arrondissement" in location_column.lower():
        coords_dict = geo_data["Arrondissement"]
        location_type = "Arrondissement"
    elif "Quartier" in location_column.lower():
        coords_dict = geo_data["Quartier"]
        location_type = "Quartier"
    else:
        # Coordonn√©es par d√©faut pour Douala, Cameroun
        coords_dict = {}
        location_type = "Localisation"
        
    # Coordonn√©es par d√©faut pour Douala, Cameroun
    default_coords = [4.0511, 9.7679]
    
    # Cr√©er une carte centr√©e sur Douala
    m = folium.Map(location=default_coords, zoom_start=zoom_start, tiles="OpenStreetMap")
    
    # Cr√©er un cluster de marqueurs pour am√©liorer les performances
    marker_cluster = MarkerCluster().add_to(m)
    
    # Ajouter des marqueurs pour chaque localisation
    locations_count = df[location_column].value_counts().to_dict()
    
    for location, count in locations_count.items():
        if location in coords_dict:
            coords = coords_dict[location]
            
            # Cr√©er le texte de popup
            popup_text = f"<strong>{location}</strong><br>Nombre de donneurs: {count}"
            
            # Ajouter des informations suppl√©mentaires si une colonne de couleur est sp√©cifi√©e
            if color_column and color_column in df.columns:
                color_values = df[df[location_column] == location][color_column].value_counts()
                popup_text += "<br><br>Distribution par " + color_column + ":<br>"
                for val, val_count in color_values.items():
                    popup_text += f"- {val}: {val_count} ({val_count/count*100:.1f}%)<br>"
            
            # Ajouter le marqueur au cluster
            folium.Marker(
                location=coords,
                popup=folium.Popup(popup_text, max_width=300),
                icon=folium.Icon(icon="info-sign", prefix='fa', color='red' if count > 50 else 'blue')
            ).add_to(marker_cluster)
    
    # Ajouter une couche de chaleur pour visualiser la densit√©
    heat_data = []
    for location, count in locations_count.items():
        if location in coords_dict:
            coords = coords_dict[location]
            # R√©p√©ter les coordonn√©es en fonction du nombre de donneurs pour cr√©er l'effet de chaleur
            for _ in range(min(count, 100)):  # Limiter √† 100 pour les performances
                heat_data.append(coords)
    
    # Ajouter la couche de chaleur si des donn√©es sont disponibles
    if heat_data:
        HeatMap(heat_data, radius=15).add_to(m)
    
    return m



# Fonction pour cr√©er un graphique de sant√© et √©ligibilit√©
@st.cache_data
def create_health_eligibility_chart(df):
    """
    Cr√©e un graphique interactif montrant l'impact des conditions de sant√© sur l'√©ligibilit√© au don.
    """
    # Identifier les colonnes de conditions de sant√©
    health_columns = [col for col in df.columns if any(term in col for term in 
                     ['Porteur', 'Op√©r√©', 'Drepanocytaire', 'Diab√©tique', 'Hypertendus', 
                      'Asthmatiques', 'Cardiaque', 'Tatou√©', 'Scarifi√©'])]
    
    # Cr√©er un DataFrame pour stocker les r√©sultats
    results = []
    
    for col in health_columns:
        # Cr√©er une table de contingence
        contingency = pd.crosstab(df['√âLIGIBILIT√â_AU_DON.'], df[col])
        
        # Calculer les pourcentages
        contingency_pct = contingency.div(contingency.sum(axis=0), axis=1) * 100
        
        # Extraire les donn√©es pour "Oui"
        if 'Oui' in contingency_pct.columns:
            for eligibility, percentage in contingency_pct['Oui'].items():
                results.append({
                    'Condition': col.split('[')[-1].split(']')[0],
                    '√âligibilit√©': eligibility,
                    'Pourcentage': percentage,
                    'Nombre': contingency.loc[eligibility, 'Oui']
                })
    
    # Cr√©er un DataFrame √† partir des r√©sultats
    results_df = pd.DataFrame(results)
    
    # Cr√©er un graphique √† barres group√©es avec Plotly
    fig = px.bar(
        results_df,
        x='Condition',
        y='Pourcentage',
        color='√âligibilit√©',
        barmode='group',
        text='Nombre',
        title="Impact des conditions de sant√© sur l'√©ligibilit√© au don",
        labels={'Pourcentage': 'Pourcentage de donneurs (%)', 'Condition': 'Condition de sant√©'},
        color_discrete_sequence=px.colors.qualitative.Set1
    )
    
    # Personnaliser le graphique
    fig.update_layout(
        xaxis_title="Condition de sant√©",
        yaxis_title="Pourcentage de donneurs (%)",
        legend_title="√âligibilit√©",
        font=dict(size=12),
        height=600
    )
    
    return fig

# Fonction pour cr√©er un graphique de clustering des donneurs
@st.cache_data
def create_donor_clustering(df):
    """
    Cr√©e une visualisation interactive des clusters de donneurs.
    """
    # S√©lectionner les variables num√©riques pour le clustering
    numeric_df = df.select_dtypes(include=['int64', 'float64'])
    
    # Afficher les informations sur les colonnes num√©riques pour le d√©bogage
    print(f"Colonnes num√©riques disponibles: {numeric_df.columns.tolist()}")
    print(f"Nombre de colonnes num√©riques: {numeric_df.shape[1]}")
    
    # S√©lectionner uniquement les colonnes sans valeurs manquantes ou avec peu de valeurs manquantes
    # pour √©viter les probl√®mes de dimensionnalit√©
    threshold = 0.5  # Colonnes avec moins de 50% de valeurs manquantes
    numeric_df = numeric_df.loc[:, numeric_df.isnull().mean() < threshold]
    print(f"Colonnes num√©riques apr√®s filtrage: {numeric_df.columns.tolist()}")
    
    if numeric_df.shape[1] >= 2:
        # G√©rer les valeurs manquantes avant le clustering
        from sklearn.impute import SimpleImputer
        imputer = SimpleImputer(strategy='median')
        
        # Imputer les valeurs manquantes
        imputed_values = imputer.fit_transform(numeric_df)
        
        # Cr√©er un nouveau DataFrame avec les valeurs imput√©es
        numeric_df_imputed = pd.DataFrame(
            imputed_values,
            columns=numeric_df.columns,
            index=numeric_df.index
        )
        
        # Standardiser les donn√©es
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(numeric_df_imputed)
        
        # Appliquer K-means avec 3 clusters
        kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
        clusters = kmeans.fit_predict(scaled_data)
        
        # Ajouter les clusters au DataFrame
        df_with_clusters = df.copy()
        df_with_clusters['Cluster'] = clusters
        
        # Appliquer PCA pour la visualisation
        pca = PCA(n_components=2)
        pca_result = pca.fit_transform(scaled_data)
        
        # Cr√©er un DataFrame pour la visualisation
        pca_df = pd.DataFrame({
            'PC1': pca_result[:, 0],
            'PC2': pca_result[:, 1],
            'Cluster': clusters.astype(str)
        })
        
        # Ajouter des informations suppl√©mentaires si disponibles
        if 'Age' in df.columns:
            pca_df['Age'] = df['Age'].values
        if 'Genre_' in df.columns:
            pca_df['Genre'] = df['Genre_'].values
        if 'Niveau_d\'etude' in df.columns:
            pca_df['Niveau_d\'√©tudes'] = df['Niveau_d\'etude'].values
        
        # Cr√©er un graphique interactif avec Plotly
        fig = px.scatter(pca_df,
            render_mode='auto', 
            x='PC1',
            y='PC2',
            color='Cluster',
            hover_data=pca_df.columns,
            title="Clustering des donneurs (PCA)",
            labels={'PC1': f'PC1 ({pca.explained_variance_ratio_[0]:.2%} de variance expliqu√©e)',
                   'PC2': f'PC2 ({pca.explained_variance_ratio_[1]:.2%} de variance expliqu√©e)'},
            color_discrete_sequence=px.colors.qualitative.Bold)
        
        # Personnaliser le graphique
        fig.update_layout(
            xaxis_title=f'PC1 ({pca.explained_variance_ratio_[0]:.2%} de variance expliqu√©e)',
            yaxis_title=f'PC2 ({pca.explained_variance_ratio_[1]:.2%} de variance expliqu√©e)',
            legend_title="Cluster",
            font=dict(size=12),
            height=600)
        
        # Analyser les caract√©ristiques de chaque cluster
        cluster_stats = df_with_clusters.groupby('Cluster').agg({
            col: ['mean', 'std'] for col in numeric_df.columns
        })
        
        return fig, cluster_stats, df_with_clusters
    else:
        return None, None, df

# Fonction pour cr√©er un graphique d'analyse de campagne
@st.cache_data
def create_campaign_analysis(df):
    """
    Cr√©e des visualisations pour analyser l'efficacit√© des campagnes de don.
    """
    # Identifier les colonnes de date
    date_columns = [col for col in df.columns if 'date' in col.lower()]
    
    # S√©lectionner une colonne de date appropri√©e
    selected_date_col = None
    for col in date_columns:
        if df[col].notna().sum() > 100:  # V√©rifier qu'il y a suffisamment de donn√©es
            selected_date_col = col
            break
    
    if selected_date_col:
        # Cr√©er une copie du DataFrame avec la colonne de date
        df_temp = df.copy()
        
        # Convertir en datetime si ce n'est pas d√©j√† fait
        df_temp[selected_date_col] = pd.to_datetime(df_temp[selected_date_col], errors='coerce')
        
        # Extraire l'ann√©e et le mois
        df_temp['year'] = df_temp[selected_date_col].dt.year
        df_temp['month'] = df_temp[selected_date_col].dt.month
        df_temp['year_month'] = df_temp[selected_date_col].dt.to_period('M')
        
        # Compter le nombre de donneurs par mois
        monthly_counts = df_temp.groupby('year_month').size().reset_index(name='count')
        monthly_counts['year_month_str'] = monthly_counts['year_month'].astype(str)
        
        # Cr√©er un graphique de tendance temporelle
        fig1 = px.line(
            monthly_counts,
            x='year_month_str',
            y='count',
            markers=True,
            title=f"√âvolution du nombre de donneurs au fil du temps (bas√© sur {selected_date_col})",
            labels={'count': 'Nombre de donneurs', 'year_month_str': 'Ann√©e-Mois'}
        )
        
        # Personnaliser le graphique
        fig1.update_layout(
            xaxis_title="P√©riode",
            yaxis_title="Nombre de donneurs",
            font=dict(size=12),
            height=500
        )
        
        # Analyser les tendances par caract√©ristiques d√©mographiques
        demographic_figs = []
        
        # Analyser par genre si disponible
        if 'Genre_' in df_temp.columns:
            gender_monthly = df_temp.groupby(['year_month', 'Genre_']).size().reset_index(name='count')
            gender_monthly['year_month_str'] = gender_monthly['year_month'].astype(str)
            
            fig_gender = px.line(
                gender_monthly,
                x='year_month_str',
                y='count',
                color='Genre_',
                markers=True,
                title="√âvolution du nombre de donneurs par genre",
                labels={'count': 'Nombre de donneurs', 'year_month_str': 'Ann√©e-Mois', 'Genre_': 'Genre'}
            )
            
            fig_gender.update_layout(
                xaxis_title="P√©riode",
                yaxis_title="Nombre de donneurs",
                legend_title="Genre",
                font=dict(size=12),
                height=400
            )
            
            demographic_figs.append(fig_gender)
        
        # Analyser par niveau d'√©tudes si disponible
        if 'Niveau_d\'etude' in df_temp.columns:
            # Simplifier les cat√©gories pour une meilleure lisibilit√©
            df_temp['Niveau_simplifi√©'] = df_temp['Niveau_d\'etude'].apply(
                lambda x: 'Universitaire' if 'Universitaire' in str(x) 
                else ('Secondaire' if 'Secondaire' in str(x)
                     else ('Primaire' if 'Primaire' in str(x)
                          else ('Aucun' if 'Aucun' in str(x) else 'Non pr√©cis√©')))
            )
            
            edu_monthly = df_temp.groupby(['year_month', 'Niveau_simplifi√©']).size().reset_index(name='count')
            edu_monthly['year_month_str'] = edu_monthly['year_month'].astype(str)
            
            fig_edu = px.line(
                edu_monthly,
                x='year_month_str',
                y='count',
                color='Niveau_simplifi√©',
                markers=True,
                title="√âvolution du nombre de donneurs par niveau d'√©tudes",
                labels={'count': 'Nombre de donneurs', 'year_month_str': 'Ann√©e-Mois', 'Niveau_simplifi√©': "Niveau d'√©tudes"}
            )
            
            fig_edu.update_layout(
                xaxis_title="P√©riode",
                yaxis_title="Nombre de donneurs",
                legend_title="Niveau d'√©tudes",
                font=dict(size=12),
                height=400
            )
            
            demographic_figs.append(fig_edu)
        
        return fig1, demographic_figs
    else:
        return None, []

# Fonction pour cr√©er une analyse de fid√©lisation des donneurs
@st.cache_data
def create_donor_retention_analysis(df):
    """
    Cr√©e des visualisations pour analyser la fid√©lisation des donneurs.
    """
    # V√©rifier si la colonne indiquant si le donneur a d√©j√† donn√© est disponible
    if 'A-t-il_(elle)_d√©j√†_donn√©_le_sang_' in df.columns:
        # Compter le nombre de donneurs qui ont d√©j√† donn√© et ceux qui n'ont pas donn√©
        retention_counts = df['A-t-il_(elle)_d√©j√†_donn√©_le_sang_'].value_counts().reset_index()
        retention_counts.columns = ['Statut', 'Nombre']
        
        # Cr√©er un graphique circulaire
        fig1 = px.pie(
            retention_counts,
            values='Nombre',
            names='Statut',
            title="Proportion de donneurs fid√©lis√©s",
            color_discrete_sequence=px.colors.qualitative.Set2
        )
        
        fig1.update_layout(
            font=dict(size=12),
            height=400
        )
        
        # Analyser la fid√©lisation par caract√©ristiques d√©mographiques
        demographic_figs = []
        
        # Analyser par genre si disponible
        if 'Genre_' in df.columns:
            gender_retention = pd.crosstab(df['Genre_'], df['A-t-il_(elle)_d√©j√†_donn√©_le_sang_'])
            gender_retention_pct = gender_retention.div(gender_retention.sum(axis=1), axis=0) * 100
            
            # Convertir en format long pour Plotly
            gender_retention_long = gender_retention_pct.reset_index().melt(
                id_vars='Genre_',
                var_name='Statut',
                value_name='Pourcentage'
            )
            
            fig_gender = px.bar(
                gender_retention_long,
                x='Genre_',
                y='Pourcentage',
                color='Statut',
                barmode='group',
                title="Fid√©lisation des donneurs par genre",
                labels={'Pourcentage': 'Pourcentage (%)', 'Genre_': 'Genre', 'Statut': 'A d√©j√† donn√©'}
            )
            
            fig_gender.update_layout(
                xaxis_title="Genre",
                yaxis_title="Pourcentage (%)",
                legend_title="A d√©j√† donn√©",
                font=dict(size=12),
                height=400
            )
            
            demographic_figs.append(fig_gender)
        
        # Analyser par niveau d'√©tudes si disponible
        if 'Niveau_d\'etude' in df.columns:
            # Simplifier les cat√©gories pour une meilleure lisibilit√©
            df['Niveau_simplifi√©'] = df['Niveau_d\'etude'].apply(
                lambda x: 'Universitaire' if 'Universitaire' in str(x) 
                else ('Secondaire' if 'Secondaire' in str(x)
                     else ('Primaire' if 'Primaire' in str(x)
                          else ('Aucun' if 'Aucun' in str(x) else 'Non pr√©cis√©')))
            )
            
            edu_retention = pd.crosstab(df['Niveau_simplifi√©'], df['A-t-il_(elle)_d√©j√†_donn√©_le_sang_'])
            edu_retention_pct = edu_retention.div(edu_retention.sum(axis=1), axis=0) * 100
            
            # Convertir en format long pour Plotly
            edu_retention_long = edu_retention_pct.reset_index().melt(
                id_vars='Niveau_simplifi√©',
                var_name='Statut',
                value_name='Pourcentage'
            )
            
            fig_edu = px.bar(
                edu_retention_long,
                x='Niveau_simplifi√©',
                y='Pourcentage',
                color='Statut',
                barmode='group',
                title="Fid√©lisation des donneurs par niveau d'√©tudes",
                labels={'Pourcentage': 'Pourcentage (%)', 'Niveau_simplifi√©': "Niveau d'√©tudes", 'Statut': 'A d√©j√† donn√©'}
            )
            
            fig_edu.update_layout(
                xaxis_title="Niveau d'√©tudes",
                yaxis_title="Pourcentage (%)",
                legend_title="A d√©j√† donn√©",
                font=dict(size=12),
                height=400
            )
            
            demographic_figs.append(fig_edu)
        
        # Analyser par √¢ge si disponible
        if 'Age' in df.columns:
            # Cr√©er des tranches d'√¢ge
            df['Tranche_√¢ge'] = pd.cut(
                df['Age'],
                bins=[0, 18, 25, 35, 45, 55, 100],
                labels=['<18', '18-25', '26-35', '36-45', '46-55', '>55']
            )
            
            age_retention = pd.crosstab(df['Tranche_√¢ge'], df['A-t-il_(elle)_d√©j√†_donn√©_le_sang_'])
            age_retention_pct = age_retention.div(age_retention.sum(axis=1), axis=0) * 100
            
            # Convertir en format long pour Plotly
            age_retention_long = age_retention_pct.reset_index().melt(
                id_vars='Tranche_√¢ge',
                var_name='Statut',
                value_name='Pourcentage'
            )
            
            fig_age = px.bar(
                age_retention_long,
                x='Tranche_√¢ge',
                y='Pourcentage',
                color='Statut',
                barmode='group',
                title="Fid√©lisation des donneurs par tranche d'√¢ge",
                labels={'Pourcentage': 'Pourcentage (%)', 'Tranche_√¢ge': "Tranche d'√¢ge", 'Statut': 'A d√©j√† donn√©'}
            )
            
            fig_age.update_layout(
                xaxis_title="Tranche d'√¢ge",
                yaxis_title="Pourcentage (%)",
                legend_title="A d√©j√† donn√©",
                font=dict(size=12),
                height=400
            )
            
            demographic_figs.append(fig_age)
        
        return fig1, demographic_figs
    else:
        return None, []

# Fonction pour cr√©er une analyse de sentiment
@st.cache_data
def create_sentiment_analysis(df):
    """
    Cr√©e des visualisations pour l'analyse de sentiment des commentaires des donneurs.
    """
    # V√©rifier si une colonne de commentaires est disponible
    comment_columns = [col for col in df.columns if any(term in col.lower() for term in 
                      ['pr√©ciser', 'raison', 'commentaire', 'feedback'])]
    
    if comment_columns:
        selected_col = comment_columns[0]
        
        # Filtrer les commentaires non vides
        comments_df = df[df[selected_col].notna() & (df[selected_col] != '')].copy()
        
        if len(comments_df) > 0:
            # T√©l√©charger les ressources NLTK si n√©cessaire
            try:
                nltk.download('vader_lexicon', quiet=True)
                
                # Initialiser l'analyseur de sentiment
                sia = SentimentIntensityAnalyzer()
                
                # Fonction pour classifier le sentiment
                def classify_sentiment(text):
                    if pd.isna(text) or text == '':
                        return 'Neutre'
                    
                    # Utiliser TextBlob pour l'analyse de sentiment en fran√ßais
                    blob = TextBlob(str(text))
                    polarity = blob.sentiment.polarity
                    
                    if polarity > 0.1:
                        return 'Positif'
                    elif polarity < -0.1:
                        return 'N√©gatif'
                    else:
                        return 'Neutre'
                
                # Appliquer l'analyse de sentiment
                comments_df['Sentiment'] = comments_df[selected_col].apply(classify_sentiment)
                
                # Compter les sentiments
                sentiment_counts = comments_df['Sentiment'].value_counts().reset_index()
                sentiment_counts.columns = ['Sentiment', 'Nombre']
                
                # Cr√©er un graphique circulaire
                fig1 = px.pie(
                    sentiment_counts,
                    values='Nombre',
                    names='Sentiment',
                    title="Analyse de sentiment des commentaires",
                    color_discrete_sequence=px.colors.qualitative.Set3
                )
                
                fig1.update_layout(
                    font=dict(size=12),
                    height=400
                )
                
                # Cr√©er un nuage de mots des commentaires les plus fr√©quents
                from wordcloud import WordCloud
                
                # Combiner tous les commentaires
                all_comments = ' '.join(comments_df[selected_col].astype(str).tolist())
                
                # Cr√©er un nuage de mots
                wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(all_comments)
                
                # Convertir le nuage de mots en image
                plt.figure(figsize=(10, 5))
                plt.imshow(wordcloud, interpolation='bilinear')
                plt.axis('off')
                plt.tight_layout()
                
                # Sauvegarder l'image
                wordcloud_path = r"C:\Users\Ultra Tech\Desktop\wordcloud.png"
                plt.savefig(wordcloud_path)
                plt.close()
                
                return fig1, wordcloud_path, comments_df
            except Exception as e:
                st.error(f"Erreur lors de l'analyse de sentiment: {e}")
                return None, None, None
        else:
            return None, None, None
    else:
        return None, None, None

def set_background(image_file):
    with open(image_file, "rb") as f:
        encoded_string = base64.b64encode(f.read()).decode()
    bg_image = f"""
    <style>
    .stApp {{
        background-image: url("data:image/png;base64,{encoded_string}");
        background-size: cover;
    }}
    </style>
    """
    return st.markdown(bg_image, unsafe_allow_html=True)



# Interface principale du tableau de bord
def main():
    #image_file=r"C:\Users\hp\Desktop\Projet dashboard\WhatsApp Image 2025-03-23 √† 22.21.36_abdc063e.jpg"
    #set_background(image_file)
    """
    Fonction principale qui cr√©e l'interface du tableau de bord Streamlit.
    """
    # Titre et introduction
    st.title("üìä Tableau de Bord d'Analyse des Donneurs de Sang")
    st.markdown("""
    Ce tableau de bord interactif pr√©sente une analyse approfondie des donn√©es de donneurs de sang,
    permettant d'optimiser les campagnes de don et d'am√©liorer la gestion des donneurs.
    """)
    
    # Charger les donn√©es
    df_2019, df_volontaire =load_data() #"df,df_volontaires"
    
    # Barre lat√©rale pour la navigation
    st.sidebar.title("Navigation")
    page = st.sidebar.radio(
        "S√©lectionnez une page",
        ["Aper√ßu des donn√©es", "Distribution g√©ographique", "Sant√© et √©ligibilit√©", 
         "Profils des donneurs", "Analyse des campagnes", "Fid√©lisation des donneurs",
         "Analyse de sentiment", "Pr√©diction d'√©ligibilit√©"]
    )
    
    # S√©lection du jeu de donn√©es
    st.sidebar.title("Jeu de donn√©es")
    dataset = st.sidebar.radio(
        "S√©lectionnez un jeu de donn√©es",
        ["2019", "Volontaire"]
    )
    
    # S√©lectionner le DataFrame en fonction du choix
    df = df_2019 if dataset == "2019" else df_volontaire
    
    # Afficher la page s√©lectionn√©e
    # Page d'accueil
    
    if page == "Aper√ßu des donn√©es":
        st.header("Aper√ßu des Donn√©es")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Nombre de donneurs (2019)", len(df_2019))
        with col2:
            st.metric("Nombre de donneurs (2020)", len(df_2020))
        with col3:
            st.metric("Nombre de donneurs (Volontaires)", len(df_volontaire))
        
        st.subheader("Aper√ßu des donn√©es")
        
        tab1, tab2, tab3 = st.tabs(["Donn√©es 2019", "Donn√©es 2020", "Donn√©es Volontaires"])
        
        with tab1:
            st.dataframe(df_2019.head())
            
            # Afficher les statistiques descriptives
            st.subheader("Statistiques descriptives")
            st.dataframe(df_2019.describe())
            
            # Visualiser les valeurs manquantes
            st.subheader("Valeurs manquantes")
            missing = df_2019.isnull().sum().sort_values(ascending=False)
            missing = missing[missing > 0]
            if len(missing) > 0:
                missing_percent = (missing / len(df_2019) * 100).round(2)
                missing_df = pd.DataFrame({'Nombre': missing, 'Pourcentage (%)': missing_percent})
                fig = px.bar(missing_df, x=missing_df.index, y='Pourcentage (%)', 
                            text='Nombre',
                            title='Valeurs manquantes - Donn√©es 2019',
                            color='Pourcentage (%)',
                            color_continuous_scale='Viridis')
                fig.update_layout(xaxis_title='Variables', yaxis_title='Pourcentage (%)')
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("Aucune valeur manquante dans les donn√©es 2019.")
        
        with tab2:
            st.dataframe(df_2020.head())
            
            # Afficher les statistiques descriptives
            st.subheader("Statistiques descriptives")
            st.dataframe(df_2020.describe())
            
            # Visualiser les valeurs manquantes
            st.subheader("Valeurs manquantes")
            missing = df_2020.isnull().sum().sort_values(ascending=False)
            missing = missing[missing > 0]
            if len(missing) > 0:
                missing_percent = (missing / len(df_2020) * 100).round(2)
                missing_df = pd.DataFrame({'Nombre': missing, 'Pourcentage (%)': missing_percent})
                fig = px.bar(missing_df, x=missing_df.index, y='Pourcentage (%)', 
                            text='Nombre',
                            title='Valeurs manquantes - Donn√©es 2020',
                            color='Pourcentage (%)',
                            color_continuous_scale='Viridis')
                fig.update_layout(xaxis_title='Variables', yaxis_title='Pourcentage (%)')
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("Aucune valeur manquante dans les donn√©es 2020.")
        
        with tab3:
            st.dataframe(df_volontaire.head())
            
            # Afficher les statistiques descriptives
            st.subheader("Statistiques descriptives")
            st.dataframe(df_volontaire.describe())
            
            # Visualiser les valeurs manquantes
            st.subheader("Valeurs manquantes")
            missing = df_volontaire.isnull().sum().sort_values(ascending=False)
            missing = missing[missing > 0]
            if len(missing) > 0:
                missing_percent = (missing / len(df_volontaire) * 100).round(2)
                missing_df = pd.DataFrame({'Nombre': missing, 'Pourcentage (%)': missing_percent})
                fig = px.bar(missing_df.head(20), x=missing_df.head(20).index, y='Pourcentage (%)', 
                            text='Nombre',
                            title='Valeurs manquantes (Top 20) - Donn√©es Volontaires',
                            color='Pourcentage (%)',
                            color_continuous_scale='Viridis')
                fig.update_layout(xaxis_title='Variables', yaxis_title='Pourcentage (%)')
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("Aucune valeur manquante dans les donn√©es Volontaires.")
    
        
            st.header("üìã Aper√ßu des donn√©es")
            
            # Afficher des statistiques g√©n√©rales
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Nombre total de donneurs", len(df))
            with col2:
                if 'Genre_' in df.columns:
                    gender_counts = df['Genre_'].value_counts()
                    st.metric("Hommes", gender_counts.get('Homme', 0))
            with col3:
                if 'Genre_' in df.columns:
                    st.metric("Femmes", gender_counts.get('Femme', 0))
            
            # Afficher la distribution par √¢ge si disponible
            if 'Age' in df.columns:
                st.subheader("Distribution par √¢ge")
                fig = px.histogram(
                    df,
                    x='Age',
                    nbins=20,
                    colors='Age',
                    title="Distribution des √¢ges des donneurs",
                    labels={'Age': '√Çge', 'count': 'Nombre de donneurs'}
                )
                st.plotly_chart(fig, use_container_width=True)
            
            # Afficher la distribution par niveau d'√©tudes si disponible
            if 'Niveau_d\'etude' in df.columns:
                st.subheader("Distribution par niveau d'√©tudes")
                edu_counts = df['Niveau_d\'etude'].value_counts().reset_index()
                edu_counts.columns = ['Niveau', 'Nombre']
                
                fig = px.bar(
                    edu_counts,
                    x='Niveau',
                    y='Nombre',
                    color='Niveau',
                    title="Distribution par niveau d'√©tudes",
                    labels={'Niveau': "Niveau d'√©tudes", 'Nombre': 'Nombre de donneurs'}
                )
                st.plotly_chart(fig, use_container_width=True)
            
            # Afficher un aper√ßu du DataFrame
            st.subheader("Aper√ßu des donn√©es brutes")
            st.dataframe(paginate_dataframe(df))
            
            
    
    elif page == "Distribution g√©ographique":
        st.header("üó∫Ô∏è Distribution g√©ographique des donneurs")
         # 1Ô∏è‚É£ Chargement des donn√©es (Fichier Excel)
        #file_path = r"C:\Users\Ultra Tech\Desktop\Challenge dataset trait√©.xlsx"  # Mets le bon chemin
        df_carte = df_volontaire.copy()
        # üîπ Dictionnaire des nouveaux noms de colonnes
        new_column_names = {
            "ID": "ID",
            "Age": "Age",
            "Horodateur": "Horodateur",
            "Niveau_d'etude": "Niveau_Etude",
            "Genre_": "Genre",
            "Taille_": "Taille",
            "Poids": "Poids",
            "Situation_Matrimoniale_(SM)": "Statut_Matrimonial",
            "Profession_": "Profession",
            "Arrondissement_de_r√©sidence_": "Arrondissement",
            "Quartier_de_R√©sidence_": "Quartier",
            "Nationalit√©_": "Nationalite",
            "Religion_": "Religion",
            "A-t-il_(elle)_d√©j√†_donn√©_le_sang_": "Deja_Donneur",
            "Si_oui_preciser_la_date_du_dernier_don._": "Date_Dernier_Don",
            "Taux_d‚Äôh√©moglobine_": "Taux_Hemoglobine",
            "√âLIGIBILIT√â_AU_DON.": "Eligibilite_Don",
            "Raison_indisponibilit√©__[Est_sous_anti-bioth√©rapie__]": "Sous_Antibiotherapie",
            "Raison_indisponibilit√©__[Taux_d‚Äôh√©moglobine_bas_]": "Hemoglobine_Bas",
            "Raison_indisponibilit√©__[date_de_dernier_Don_<_3_mois_]": "Dernier_Don_3Mois",
            "Raison_indisponibilit√©__[IST_r√©cente_(Exclu_VIH,_Hbs,_Hcv)]": "IST_Recente",
            "Date_de_derni√®res_r√®gles_(DDR)__": "DDR",
            "Raison_de_l‚Äôindisponibilit√©_de_la_femme_[La_DDR_est_mauvais_si_<14_jour_avant_le_don]": "DDR_Mauvaise",
            "Raison_de_l‚Äôindisponibilit√©_de_la_femme_[Allaitement_]": "Allaitement",
            "Raison_de_l‚Äôindisponibilit√©_de_la_femme_[A_accoucher_ces_6_derniers_mois__]": "Accouchement_6Mois",
            "Raison_de_l‚Äôindisponibilit√©_de_la_femme_[Interruption_de_grossesse__ces_06_derniers_mois]": "Interruption_Grossesse",
            "Raison_de_l‚Äôindisponibilit√©_de_la_femme_[est_enceinte_]": "Enceinte",
            "Autre_raisons,__preciser_": "Autres_Raisons",
            'S√©lectionner_"ok"_pour_envoyer_': "Confirmation_OK",
            "Raison_de_non-eligibilit√©_totale__[Ant√©c√©dent_de_transfusion]": "Transfusion_Antecedent",
            "Raison_de_non-eligibilit√©_totale__[Porteur(HIV,hbs,hcv)]": "Porteur_VIH_HBS_HCV",
            "Raison_de_non-eligibilit√©_totale__[Op√©r√©]": "Opere",
            "Raison_de_non-eligibilit√©_totale__[Drepanocytaire]": "Drepanocytose",
            "Raison_de_non-eligibilit√©_totale__[Diab√©tique]": "Diabete",
            "Raison_de_non-eligibilit√©_totale__[Hypertendus]": "Hypertension",
            "Raison_de_non-eligibilit√©_totale__[Asthmatiques]": "Asthme",
            "Raison_de_non-eligibilit√©_totale__[Cardiaque]": "Probleme_Cardiaque",
            "Raison_de_non-eligibilit√©_totale__[Tatou√©]": "Tatouage",
            "Raison_de_non-eligibilit√©_totale__[Scarifi√©]": "Scarification",
            "Si_autres_raison_pr√©ciser_": "Autres_Raisons_Precises"
        }
        
        # üîπ Renommage des colonnes du DataFrame
        df_carte.rename(columns=new_column_names, inplace=True)

        
        
        # 2Ô∏è‚É£ Initialisation du G√©ocodeur avec timeout
        geolocator = Nominatim(user_agent="blood_donation_cameroon", timeout=10)
        geocode = RateLimiter(geolocator.geocode, min_delay_seconds=2, max_retries=3)
        
        # 3Ô∏è‚É£ Charger le cache si existant
        cache_file = "geo_cache.json"
        if os.path.exists(cache_file):
            with open(cache_file, "r") as f:
                geo_cache = json.load(f)
        else:
            geo_cache = {}
        
        
        modalites_indesirables = [
                "Pas precise", "Pas pr√©cis√©", "Pas precise", "Pas pr√©cise","Non pr√©cis√©","non pr√©cis√©",
                " RAS", "Ras", "R A S ", "ras", "r a s"," ","nan",
                " Pas mentionn√©", "Pasmentionn√©", "Pas mentionne"
            ]
        # 4Ô∏è‚É£ Fonction pour obtenir les coordonn√©es avec cache
        def get_coordinates_with_cache(row):
            query = f"{row['Quartier']}, {row['Arrondissement']}, Douala, Cameroun"
        
            # V√©rifier si l'adresse est d√©j√† dans le cache
            if query in geo_cache:
                return pd.Series(geo_cache[query])
        
            # V√©rifier si les donn√©es sont valides avant d'envoyer la requ√™te
            if pd.isna(row["Quartier"]) or pd.isna(row["Arrondissement"]) or row["Quartier"] in modalites_indesirables :
                return pd.Series({'latitude': 4.0483, 'longitude': 9.7043})  # Coordonn√©es par d√©faut de Douala
        
            try:
                location = geocode(query)
                if location:
                    geo_cache[query] = {'latitude': location.latitude, 'longitude': location.longitude}
                else:
                    geo_cache[query] = {'latitude': 4.0483, 'longitude': 9.7043}  # Valeur par d√©faut
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur pour {query}: {e}")
                geo_cache[query] = {'latitude': 4.0483, 'longitude': 9.7043}  # Valeur par d√©faut
        
            return pd.Series(geo_cache[query])
        
        # 5Ô∏è‚É£ V√©rification et ex√©cution du g√©ocodage uniquement si n√©cessaire
        if "latitude" in df_carte.columns and "longitude" in df_carte.columns:
            print("üìå Coordonn√©es d√©j√† pr√©sentes dans le fichier. G√©ocodage non n√©cessaire.")
        else:
            print("üìå D√©marrage du g√©ocodage avec cache...")
            tqdm.pandas()  # Activation de la barre de progression
            coordinates = df_carte.progress_apply(get_coordinates_with_cache, axis=1)
        
            # Fusionner les nouvelles coordonn√©es avec les donn√©es existantes
            df_carte = pd.concat([df_carte, coordinates], axis=1)
        
            # Sauvegarder les r√©sultats pour √©viter de refaire le g√©ocodage
            with open(cache_file, "w") as f:
                json.dump(geo_cache, f)
            print("‚úÖ G√©ocodage termin√© et sauvegard√© !")
        
        # 6Ô∏è‚É£ Cr√©ation de la carte Folium
        m = folium.Map(location=[4.0483, 9.7043], zoom_start=12, tiles='cartodb dark_matter')
        
        # 7Ô∏è‚É£ Ajout des marqueurs pour chaque donneur
        for idx, row in df_carte.iterrows():
            popup_content = f"""
            <b>{row['Genre']}, {row['Age']} ans</b><br>
            <i>{row['Profession']}</i><br>
            √âligible: {row.get('Eligibilite_Don', 'N/A')}<br>
            Quartier: {row['Quartier']}
            """
        
            folium.Marker(
                location=[row['latitude'], row['longitude']],
                popup=folium.Popup(popup_content, max_width=250),
                icon=folium.Icon(
                    color='green' if row.get('Eligibilite_Don') == 'Oui' else 'red',
                    icon='tint',
                    prefix='fa'
                )
            ).add_to(m)

        # Ajouter une couche de chaleur pour visualiser la densit√©
        location=[row['latitude'], row['longitude']]
        # Avec pond√©ration (par exemple, selon le nombre de dons)
        heat_data_weighted = []
        for idx, row in df_carte.iterrows():
            if not pd.isna(row['latitude']) and not pd.isna(row['longitude']):
                # Ajouter un poids (par exemple, nombre de dons)
                weight = row.get('Nombre_Dons', 1)  # Utiliser 1 comme valeur par d√©faut
                heat_data_weighted.append([row['latitude'], row['longitude'], weight])
        
        # Ajouter la couche de chaleur pond√©r√©e
        HeatMap(heat_data_weighted, radius=15, blur=10).add_to(m)

        from folium.plugins import MarkerCluster
        from folium import Choropleth, GeoJson
        
        # Supposons que vous avez un fichier GeoJSON avec les limites des quartiers
        # et un DataFrame avec le nombre de dons par quartier
        
        # 1. Charger le GeoJSON des quartiers
        def create_quartier_choropleth(df):
            # Cr√©er une carte centr√©e sur Douala
            #m = folium.Map(location=[4.0483, 9.7043], zoom_start=12, tiles='CartoDB positron')
            m = folium.Map(location=[4.0483, 9.7043], zoom_start=12, tiles=None)
        
            # Ajouter une couche blanche personnalis√©e
            folium.raster_layers.TileLayer(
                tiles='',
                attr='',
                name='fond blanc',
                overlay=False,
                control=True,
                opacity=1.0,
                styles=[('background-color', '#ffffff')]
            ).add_to(m)
        
            
            # Compter le nombre de dons par quartier
            quartier_counts = df['Quartier'].value_counts().reset_index()
            quartier_counts.columns = ['Quartier', 'Nombre_Dons']
            
            # Cr√©er un dictionnaire pour le style de chaque quartier
            quartier_data = quartier_counts.set_index('Quartier')['Nombre_Dons'].to_dict()
            
            # Charger le GeoJSON des quartiers (vous devez cr√©er ou obtenir ce fichier)
            try:
                with open('data/quartiers_douala.geojson', 'r') as f:
                    quartiers_geo = json.load(f)
                    
                # Cr√©er la couche Choropleth
                Choropleth(
                    geo_data=quartiers_geo,
                    name='Densit√© de dons par quartier',
                    data=quartier_counts,
                    columns=['Quartier', 'Nombre_Dons'],
                    key_on='feature.properties.name',  # Doit correspondre √† la propri√©t√© dans le GeoJSON
                    fill_color='YlOrRd',
                    fill_opacity=0.7,
                    line_opacity=0.2,
                    legend_name='Nombre de dons',
                    highlight=True,
                    tooltip=folium.features.GeoJsonTooltip(
                        fields=['name', 'Nombre_Dons'],
                        aliases=['Quartier: ', 'Nombre de dons: '],
                        localize=True
                    )
                ).add_to(m)
                
                # Ajouter un contr√¥le de couches
                folium.LayerControl().add_to(m)
                
            except FileNotFoundError:
                # Si le fichier GeoJSON n'existe pas, cr√©er une alternative
                st.warning("Fichier GeoJSON des quartiers non trouv√©. Cr√©ation d'une visualisation alternative.")
                
                # Alternative: utiliser des cercles proportionnels
                for quartier, count in quartier_data.items():
                    # Obtenir les coordonn√©es du quartier depuis le fichier JSON
                    geo_data = load_geo_coordinates()
                    if "Quartier" in geo_data and quartier in geo_data["Quartier"]:
                        coords = geo_data["Quartier"][quartier]
                        
                        # Cr√©er un cercle dont la taille d√©pend du nombre de dons
                        folium.Circle(
                            location=coords,
                            radius=count * 20,  # Rayon proportionnel au nombre de dons
                            color='red',
                            fill=True,
                            fill_color='red',
                            fill_opacity=0.6,
                            tooltip=f"Quartier: {quartier}<br>Nombre de dons: {count}"
                        ).add_to(m)
            
        
               
                
        # 8Ô∏è‚É£ Affichage de la carte
      
        folium_static(m)

    
    elif page == "Sant√© et √©ligibilit√©":
        st.header("ü©∫ Conditions de sant√© et √©ligibilit√© au don")
        
        if '√âLIGIBILIT√â_AU_DON.' in df.columns:
            # Afficher des statistiques g√©n√©rales sur l'√©ligibilit√©
            st.subheader("R√©partition de l'√©ligibilit√© au don")
            eligibility_counts = df['√âLIGIBILIT√â_AU_DON.'].value_counts().reset_index()
            eligibility_counts.columns = ['Statut', 'Nombre']
            
            fig = px.pie(
                eligibility_counts,
                values='Nombre',
                names='Statut',
                title="R√©partition de l'√©ligibilit√© au don",
                color_discrete_sequence=px.colors.qualitative.Set1
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
            # Cr√©er un graphique montrant l'impact des conditions de sant√© sur l'√©ligibilit√©
            st.subheader("Impact des conditions de sant√© sur l'√©ligibilit√©")
            health_fig = create_health_eligibility_chart(df)
            st.plotly_chart(health_fig, use_container_width=True)
            
            # Analyser l'impact des facteurs d√©mographiques sur l'√©ligibilit√©
            st.subheader("Impact des facteurs d√©mographiques sur l'√©ligibilit√©")
            
            # S√©lectionner le facteur d√©mographique
            demo_columns = [col for col in df.columns if any(term in col for term in 
                           ['Genre', 'Age', 'Niveau', 'Situation', 'Profession'])]
            
            if demo_columns:
                demo_col = st.selectbox(
                    "S√©lectionnez un facteur d√©mographique",
                    demo_columns
                )
                
                # Cr√©er une table de contingence
                contingency = pd.crosstab(df[demo_col], df['√âLIGIBILIT√â_AU_DON.'])
                
                # Calculer les pourcentages par ligne
                contingency_pct = contingency.div(contingency.sum(axis=1), axis=0) * 100
                
                # Convertir en format long pour Plotly
                contingency_long = contingency_pct.reset_index().melt(
                    id_vars=demo_col,
                    var_name='√âligibilit√©',
                    value_name='Pourcentage'
                )
                
                fig = px.bar(
                    contingency_long,
                    x=demo_col,
                    y='Pourcentage',
                    color='√âligibilit√©',
                    barmode='group',
                    title=f"√âligibilit√© au don par {demo_col}",
                    labels={'Pourcentage': 'Pourcentage (%)', demo_col: demo_col, '√âligibilit√©': '√âligibilit√©'}
                )
                
                fig.update_layout(
                    xaxis_title=demo_col,
                    yaxis_title="Pourcentage (%)",
                    legend_title="√âligibilit√©",
                    font=dict(size=12),
                    height=500,
                    xaxis={'categoryorder': 'total descending'}
                )
                
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.warning("Aucun facteur d√©mographique identifi√© dans les donn√©es.")
        else:
            st.warning("La colonne d'√©ligibilit√© au don n'est pas disponible dans ce jeu de donn√©es.")
    
    elif page == "Profils des donneurs":
        st.header("üë• Profils des donneurs")
        
        # Effectuer le clustering des donneurs
        cluster_fig, cluster_stats, df_with_clusters = create_donor_clustering(df)
        
        if cluster_fig is not None:
            # Afficher la visualisation des clusters
            st.subheader("Clustering des donneurs")
            st.plotly_chart(cluster_fig, use_container_width=True)
            
            # Afficher les caract√©ristiques des clusters
            st.subheader("Caract√©ristiques des clusters")
            st.dataframe(cluster_stats)
            
            # Cr√©er des profils de donneurs id√©aux
            st.subheader("Profils de donneurs id√©aux")
            
            # Identifier le cluster avec le plus grand nombre de donneurs √©ligibles
            if '√âLIGIBILIT√â_AU_DON.' in df_with_clusters.columns and 'Cluster' in df_with_clusters.columns:
                # Compter le nombre de donneurs √©ligibles par cluster
                eligible_counts = df_with_clusters[df_with_clusters['√âLIGIBILIT√â_AU_DON.'] == 'Eligible'].groupby('Cluster').size()
                
                if not eligible_counts.empty:
                    ideal_cluster = eligible_counts.idxmax()
                    
                    st.write(f"**Cluster id√©al identifi√©: Cluster {ideal_cluster}**")
                    
                    # Extraire les caract√©ristiques du cluster id√©al
                    ideal_profile = df_with_clusters[df_with_clusters['Cluster'] == ideal_cluster]
                    
                    # Afficher les caract√©ristiques d√©mographiques du cluster id√©al
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.write("**Caract√©ristiques d√©mographiques:**")
                        
                        if 'Age' in ideal_profile.columns:
                            st.write(f"- √Çge moyen: {ideal_profile['Age'].mean():.1f} ans")
                        
                        if 'Genre_' in ideal_profile.columns:
                            gender_pct = ideal_profile['Genre_'].value_counts(normalize=True) * 100
                            st.write(f"- Genre: {gender_pct.get('Homme', 0):.1f}% Hommes, {gender_pct.get('Femme', 0):.1f}% Femmes")
                        
                        if 'Niveau_d\'etude' in ideal_profile.columns:
                            top_edu = ideal_profile['Niveau_d\'etude'].value_counts(normalize=True).head(2)
                            st.write("- Niveau d'√©tudes principal:")
                            for edu, pct in top_edu.items():
                                st.write(f"  ‚Ä¢ {edu}: {pct*100:.1f}%")
                        
                        if 'Situation_Matrimoniale_(SM)' in ideal_profile.columns:
                            top_marital = ideal_profile['Situation_Matrimoniale_(SM)'].value_counts(normalize=True).head(2)
                            st.write("- Situation matrimoniale principale:")
                            for status, pct in top_marital.items():
                                st.write(f"  ‚Ä¢ {status}: {pct*100:.1f}%")
                    
                    with col2:
                        st.write("**Caract√©ristiques g√©ographiques:**")
                        
                        geo_columns = [col for col in ideal_profile.columns if any(term in col for term in 
                                      ['Arrondissement', 'Quartier', 'R√©sidence'])]
                        
                        for geo_col in geo_columns:
                            top_geo = ideal_profile[geo_col].value_counts(normalize=True).head(3)
                            st.write(f"- {geo_col} principal:")
                            for zone, pct in top_geo.items():
                                st.write(f"  ‚Ä¢ {zone}: {pct*100:.1f}%")
                    
                    # Cr√©er un radar chart pour visualiser le profil id√©al
                    if 'Age' in ideal_profile.columns and 'Taille_' in ideal_profile.columns and 'Poids' in ideal_profile.columns:
                        # Calculer les moyennes normalis√©es
                        avg_age = ideal_profile['Age'].mean() / df['Age'].max()
                        avg_height = ideal_profile['Taille_'].mean() / df['Taille_'].max()
                        avg_weight = ideal_profile['Poids'].mean() / df['Poids'].max()
                        
                        # Cr√©er un radar chart
                        categories = ['√Çge', 'Taille', 'Poids']
                        values = [avg_age, avg_height, avg_weight]
                        
                        fig = go.Figure()
                        
                        fig.add_trace(go.Scatterpolar(
                            r=values,
                            theta=categories,
                            fill='toself',
                            name='Profil id√©al'
                        ))
                        
                        fig.update_layout(
                            polar=dict(
                                radialaxis=dict(
                                    visible=True,
                                    range=[0, 1]
                                )
                            ),
                            title="Caract√©ristiques physiques du profil id√©al (normalis√©es)",
                            height=400
                        )
                        
                        st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("Impossible de d√©terminer le profil id√©al car la colonne d'√©ligibilit√© n'est pas disponible.")
        else:
            st.warning("Impossible d'effectuer le clustering car il n'y a pas assez de variables num√©riques dans les donn√©es.")
    
    elif page == "Analyse des campagnes":
        st.header("üìà Analyse des campagnes de don")
        
        # Cr√©er des visualisations pour l'analyse des campagnes
        campaign_fig, demographic_figs = create_campaign_analysis(df)
        
        if campaign_fig is not None:
            # Afficher la tendance temporelle g√©n√©rale
            st.subheader("√âvolution du nombre de donneurs au fil du temps")
            st.plotly_chart(campaign_fig, use_container_width=True)
            
            # Afficher les tendances par caract√©ristiques d√©mographiques
            if demographic_figs:
                st.subheader("Tendances par caract√©ristiques d√©mographiques")
                
                for fig in demographic_figs:
                    st.plotly_chart(fig, use_container_width=True)
            
            # Ajouter des recommandations pour l'optimisation des campagnes
            st.subheader("Recommandations pour l'optimisation des campagnes")
            
            st.write("""
            Sur la base de l'analyse des donn√©es, voici quelques recommandations pour optimiser les futures campagnes de don de sang:
            
            1. **Ciblage d√©mographique**: Concentrez les efforts sur les segments de population les plus susceptibles de donner, comme identifi√© dans l'analyse des profils.
            
            2. **Planification temporelle**: Organisez les campagnes pendant les p√©riodes o√π le taux de participation est historiquement √©lev√©.
            
            3. **Localisation g√©ographique**: Privil√©giez les zones g√©ographiques avec un taux d'√©ligibilit√© √©lev√© et une forte concentration de donneurs potentiels.
            
            4. **Sensibilisation cibl√©e**: D√©veloppez des messages sp√©cifiques pour les groupes sous-repr√©sent√©s afin d'augmenter leur participation.
            
            5. **Fid√©lisation**: Mettez en place des strat√©gies pour encourager les donneurs √† revenir r√©guli√®rement.
            """)
        else:
            st.warning("Impossible d'analyser les tendances temporelles car aucune colonne de date appropri√©e n'a √©t√© identifi√©e.")
    
    elif page == "Fid√©lisation des donneurs":
        st.header("üîÑ Fid√©lisation des donneurs")
        
        # Cr√©er des visualisations pour l'analyse de fid√©lisation
        retention_fig, demographic_figs = create_donor_retention_analysis(df)
        
        if retention_fig is not None:
            # Afficher la proportion de donneurs fid√©lis√©s
            st.subheader("Proportion de donneurs fid√©lis√©s")
            st.plotly_chart(retention_fig, use_container_width=True)
            
            # Afficher la fid√©lisation par caract√©ristiques d√©mographiques
            if demographic_figs:
                st.subheader("Fid√©lisation par caract√©ristiques d√©mographiques")
                
                for fig in demographic_figs:
                    st.plotly_chart(fig, use_container_width=True)
            
            # Ajouter des strat√©gies pour am√©liorer la fid√©lisation
            st.subheader("Strat√©gies pour am√©liorer la fid√©lisation des donneurs")
            
            st.write("""
            Voici quelques strat√©gies pour am√©liorer la fid√©lisation des donneurs de sang:
            
            1. **Programme de reconnaissance**: Mettre en place un syst√®me de reconnaissance pour les donneurs r√©guliers (badges, certificats, etc.).
            
            2. **Communication personnalis√©e**: Envoyer des rappels personnalis√©s aux donneurs en fonction de leur historique de don.
            
            3. **Exp√©rience positive**: Am√©liorer l'exp√©rience du donneur pendant le processus de don pour encourager le retour.
            
            4. **√âducation continue**: Informer les donneurs sur l'impact de leur don et l'importance de donner r√©guli√®rement.
            
            5. **√âv√©nements communautaires**: Organiser des √©v√©nements sp√©ciaux pour les donneurs r√©guliers afin de renforcer leur engagement.
            """)
        else:
            st.warning("Impossible d'analyser la fid√©lisation car les informations n√©cessaires ne sont pas disponibles dans les donn√©es.")
    
    elif page == "Analyse de sentiment":
        st.header("üí¨ Analyse de sentiment des retours")
        nltk.download("vader_lexicon")
        sia = SentimentIntensityAnalyzer()
        # Cr√©er des visualisations pour l'analyse de sentiment
        sentiment_fig, wordcloud_path, comments_df = create_sentiment_analysis(df)
        if "Si_autres_raison_pr√©ciser_" in df_volontaires.columns:
            df_volontaires["Sentiment"] = df_volontaires["Si_autres_raison_pr√©ciser_"].dropna().apply(lambda x: sia.polarity_scores(str(x))["compound"])
            sentiment_counts = df_volontaires["Sentiment"].apply(lambda x: "Positif" if x > 0 else "N√©gatif" if x < 0 else "Neutre").value_counts()
            fig = px.pie(sentiment_counts, names=sentiment_counts.index, title="R√©partition des sentiments")
            st.plotly_chart(fig)
            text = " ".join(str(f) for f in df_volontaires["Si_autres_raison_pr√©ciser_"].dropna())
            wordcloud = WordCloud(width=800, height=400, background_color="white").generate(text)
            st.image(wordcloud.to_array(), caption="Nuage de Mots des Feedbacks", use_column_width=True)
        
        
        
            
    
    elif page == "Pr√©diction d'√©ligibilit√©":
        st.header("üîÆ Pr√©diction d'√©ligibilit√© au don")
        
        st.title("üîç Pr√©diction d'√âligibilit√© au Don de Sang")
        # ==============================
        # üéØ CHARGEMENT DU MOD√àLE & ENCODEURS
        # ==============================
        @st.cache_resource
        def load_model():
            pathse="eligibility_model.pkl"
            data = joblib.load(pathse)
            return data["model"], data["X_test"], data["y_test"], data["target_encoder"], data["lpreprocessor"],data["resultat"]

        model, X_test, y_test, target_encoder, preprocessor,resultat = load_model()
        

        columns_binary = [
                "Drepanocytose", "Opere", "Transfusion_Antecedent", "Diabete",
                "Hypertension", "Porteur_VIH_HBS_HCV", "Asthme",
                "Probleme_Cardiaque", "Tatouage", "Scarification", "Deja_Donneur"
            ]



        # ==============================
        # üóÇ ORGANISATION EN ONGLETS
        # ==============================
        tab1, tab2,tab3= st.tabs([ "üîÑ Pr√©diction Individuelle","üì• T√©l√©charger/Charger un Fichier","Performance du mod√®le"])


        with tab1:
            col1,col2,col3=st.columns(3)
            st.subheader("üîÑ Faire une pr√©diction individuelle")

            st.write("""Ce mod√®le pr√©dit si un donneur est √©ligible ou non en fonction de ses caract√©ristiques m√©dicales et personnelles.
            Remplissez les informations ci-dessous pour obtenir une pr√©diction.
        """)
            #df=pd.read_csv(patr)
            professions = ["Etudiant (e)", "Sans Emplois", "Tailleur", "Militaire", "Bijoutier", "Couturier","Jeune cadre", "Commer√ßant (e)", "Mototaximan", "Agent commercial", "El√®ve","Chauffeur", "COIFFEUSE", "M√©canicien", "Cultuvateur", "Fonctionnaires", "Marin", 
    "Infirmi√®r", "Footballeur", "Agent de securite", "Electronicien", "√âlectricien", 
    "√âl√®ve", "Sportifs", "Personnel de sante", "Formateur", "Trader indep", 
    "Charg√© de client√®le", "CONTROLEUR DES DOUANES", "ASSISTANT DE  DIRECTION", 
    "STAGIAIRE", "Angent de securit√©", "Pasteur", "S1p", "Plombier", "Security officer", 
    "BUSINESMAN", "Footballeur ", "Technicien sup√©rieur d‚Äôagriculture", "Vigil", 
    "Coordiste", "PEINTRE", "ADMINISTRATEUR DES HOPITAUX", "chauffeur", "H√¥telier ", 
    "Logisticien transport", "CHAUDRONIER", "Decorateur Baptiment", "T√¢cheron", 
    "Cuisinier", "Imprimeur ", "missionnaire", "Patissier", "Docker", "R√©alisateur ", 
    "assureur", "CHAUFFEUR ", "LAVEUR", "Coach kuine", "Conducteur", "Technicien ", 
    "Conseiller client", "Entrepreneur", "Magasinier ", "constructeur en b√¢timent", 
    "Calier", "SOUDEUR", "AGENT D'APPUI PHARMICIE", "CHAUFFEUR", "Intendant ", 
    "conducteurs d'engins genie civil", "Chauffeur ", "Assistant transit", 
    "Agent des ressourses humaines", "Declarant en Douane", "Menuisier", "AIDE COMPTABLE", 
    "TECHNICIEN GENIE CIVIL", "Transitaire", "Coiffeur", "M√©nag√®re", "VENDEUSE", 
    "m√©decin", "couturi√®re", "Pompiste", "EDUCATEUR DES ENFANTS", "Hoteliere", 
    "OUVRIER", "D√©brouillard", "MACHINISTE", "FORREUR", "CASINO", "TECHNICIEN TOPO", 
    "COUTURIERE", "RAS", "APPRENTI TOLERIE ", "SP", "D√©veloppeur en informatique ", 
    "S√©rigraphie", "Estheticien", "Maintenancier industriel ", "Auditeur interne", 
    "Enseignant (e)", "Agent municipal ", "Tolier", "Agent de banque", "Prestataire de service et consultant sportif", 
    "Dolker ", "photographe", "Agent d'exploitation", "Cheminot", "ARGENT DE S√âCURIT√â ", 
    "Secr√©taire comptable", "Contractuel d'administration", "Technicien de g√©nie civile", 
    "Juriste", "Informaticien ", "Technicien en genie civil", "Agent administratif ", 
    "Comptable", "Laborantin", "Ing√©nieur g√©nie civil", "Analyste -programmeur", 
    "Logisticien", "Agent de securit√©", "Ma√ßon", "Menuisier ", "MENUSIER", "MENUISIER ", 
    "Plombier", "Bijoutier", "Soudeur", "Peintre", "Chaudronnier", "√âlectronicien ", 
    "Electricien", "Machiniste", "P√¢tissier ", "Menuisier", "CHAUDRONNIER", 
    "Technicien g√©nie civil", "Agent technique", "Technicien r√©seaux t√©l√©coms", 
    "Infographe", "Architecte", "Assistante", "M√©nag√®re", "Commer√ßant (e)", 
    "Employ√© (e)  dans une entreprise", "Agent de s√©curit√©", "Marin", "D√©brouillard", 
    "Personnel de sante", "Comptable", "Enseignant", "Fonctionnaires", "Magasinier", 
    "Agent commercial", "Technicien", "Informaticien", "Electricien auto", 
    "Technicien de g√©nie civile", "Technicien d'agriculture", "Technicien en b√¢timent", 
    "Technicien en √©lectricit√©", "Technicien en m√©canique", "Technicien en plomberie", 
    "Technicien en soudure", "Technicien en informatique", "Technicien en g√©nie civil", 
    "Technicien en √©lectronique", "Technicien en climatisation", "Technicien en t√©l√©coms", 
    "Technicien en topographie", "Technicien en maintenance", "Technicien en chauffage", 
    "Technicien en froid", "Technicien en √©lectricit√© b√¢timent", "Technicien en √©lectricit√© industrielle", 
    "Technicien en m√©canique auto", "Technicien en m√©canique industrielle", 
    "Technicien en plomberie sanitaire", "Technicien en soudure industrielle", 
    "Technicien en informatique bureautique", "Technicien en informatique r√©seau", 
    "Technicien en informatique d√©veloppement", "Technicien en g√©nie civil b√¢timent", 
    "Technicien en g√©nie civil travaux publics", "Technicien en √©lectronique num√©rique", 
    "Technicien en √©lectronique analogique", "Technicien en climatisation centrale", 
    "Technicien en t√©l√©coms r√©seau", "Technicien en topographie terrestre", 
    "Technicien en maintenance industrielle", "Technicien en chauffage central", 
    "Technicien en froid industriel", "Technicien en √©lectricit√© b√¢timent r√©sidentiel", 
    "Technicien en √©lectricit√© industrielle lourde", "Technicien en m√©canique automobile", 
    "Technicien en m√©canique industrielle lourde", "Technicien en plomberie sanitaire r√©sidentielle", 
    "Technicien en soudure industrielle lourde", "Technicien en informatique bureautique avanc√©e", 
    "Technicien en informatique r√©seau avanc√©", "Technicien en informatique d√©veloppement avanc√©", 
    "Technicien en g√©nie civil b√¢timent avanc√©", "Technicien en g√©nie civil travaux publics avanc√©s", 
    "Technicien en √©lectronique num√©rique avanc√©e", "Technicien en √©lectronique analogique avanc√©e", 
    "Technicien en climatisation centrale avanc√©e", "Technicien en t√©l√©coms r√©seau avanc√©", 
    "Technicien en topographie terrestre avanc√©e", "Technicien en maintenance industrielle avanc√©e", 
    "Technicien en chauffage central avanc√©", "Technicien en froid industriel avanc√©", 
    "Technicien en √©lectricit√© b√¢timent r√©sidentiel avanc√©", "Technicien en √©lectricit√© industrielle lourde avanc√©e", 
    "Technicien en m√©canique automobile avanc√©e", "Technicien en m√©canique industrielle lourde avanc√©e", 
    "Technicien en plomberie sanitaire r√©sidentielle avanc√©e", "Technicien en soudure industrielle lourde avanc√©e", 
    "Technicien en informatique bureautique expert", "Technicien en informatique r√©seau expert", 
    "Technicien en informatique d√©veloppement expert", "Technicien en g√©nie civil b√¢timent expert", 
    "Technicien en g√©nie civil travaux publics expert", "Technicien en √©lectronique num√©rique expert", 
    "Technicien en √©lectronique analogique expert", "Technicien en climatisation centrale expert", 
    "Technicien en t√©l√©coms r√©seau expert", "Technicien en topographie terrestre expert", 
    "Technicien en maintenance industrielle expert", "Technicien en chauffage central expert", 
    "Technicien en froid industriel expert", "Technicien en √©lectricit√© b√¢timent r√©sidentiel expert", 
    "Technicien en √©lectricit√© industrielle lourde expert", "Technicien en m√©canique automobile expert", 
    "Technicien en m√©canique industrielle lourde expert", "Technicien en plomberie sanitaire r√©sidentielle expert", 
    "Technicien en soudure industrielle lourde expert"
]
            religions = [
    "Chretien (Catholique)", "Musulman", "Pas Pr√©cis√©", "Chretien (Protestant )", 
    "Adventiste", "Non-croyant", "pentec√¥tiste", "Chretien (Ne de nouveau)", 
    "Traditionaliste", "BAPTISTE", "pentec√¥tiste", "Chr√©tien non pr√©cis√©", 
    "pentecotiste", "Chretien (Protestant)", "Musulmane"
]
            niveaux_etude = [
    "Universitaire", "Aucun", "Secondaire", "Primaire", "Pas Pr√©cis√©"
]
            statuts_matrimoniaux = [
    "C√©libataire", "Mari√© (e)", "veuf (veuve)", "Divorc√©(e)"
]
            # ==============================
            # üìå FORMULAIRE DE SAISIE
            # ==============================
            with col1:
                age = st.number_input("√Çge", min_value=18, max_value=100, value=30, step=1)

                profession = st.selectbox("Profession",list(professions) )
                religion = st.selectbox("Religion", list(religions))
                Niveau_Etude =st.selectbox("Niveau_Etude", list(niveaux_etude))
                Statut_Matrimonial= st.selectbox("Statut_Matrimonial", list(statuts_matrimoniaux))
                # √âtat de sant√© (Binaire : Oui/Non)
            columns_binary = [
                "Drepanocytose", "Opere", "Transfusion_Antecedent", "Diabete",
                "Hypertension", "Porteur_VIH_HBS_HCV", "Asthme",
                "Probleme_Cardiaque", "Tatouage", "Scarification", "Deja_Donneur"
            ]
            columns_binarys=[
                "Drepanocytose", "Opere", "Transfusion_Antecedent", "Diabete",
                "Hypertension", "Porteur_VIH_HBS_HCV"]
            columns_binaryss=["Asthme",
                "Probleme_Cardiaque", "Tatouage", "Scarification", "Deja_Donneur"]
            
            with col2:
                binary_inputs = {}
                for col in columns_binarys:
                    binary_inputs[col] = st.radio(col, ["Non", "Oui"])
            with col3:
                for col in columns_binaryss:
                    binary_inputs[col] = st.radio(col, ["Non", "Oui"])

            # ==============================
            # üîÑ PR√âPARATION DES DONN√âES
            # ==============================
            # Convertir les entr√©es utilisateur en dataframe
            input_data = pd.DataFrame([[age, profession] + [binary_inputs[col] for col in columns_binary]+[religion,Niveau_Etude,Statut_Matrimonial]],
                                    columns=["Age", "Profession"] + columns_binary+["Religion","Niveau_Etude","Statut_Matrimonial"])

            # Encoder Profession et Religion
            input_data =preprocessor.transform(input_data)
            # ==============================
            # üöÄ PR√âDICTION
            # ==============================
            if st.button("Pr√©dire l'√©ligibilit√©"):
                prediction = model.predict(input_data)[0]
                result = target_encoder.inverse_transform([prediction])[0]
                 # Afficher des recommandations en fonction de la pr√©diction
                st.subheader("Recommandations")
                


                # Affichage du r√©sultat
                if prediction == 1:
                    st.success("‚úÖ Le donneur est *√âLIGIBLE* au don de sang !")
                    st.balloons()
                else:
                    st.error("‚ùå Le donneur *N'EST PAS √âLIGIBLE* au don de sang.")

                # Affichage des valeurs d'entr√©e encod√©es
                st.subheader("üîé Donn√©es encod√©es utilis√©es pour la pr√©diction :")
                #st.dataframe(input_data)
        
        
        # ==============================
        # üì• ONGLET : T√âL√âCHARGEMENT / UPLOAD
        # ==============================
        with tab2:

            # ==============================
            # üì• T√âL√âCHARGEMENT DU FICHIER MOD√àLE
            # ==============================
            st.subheader("üì• T√©l√©charger le mod√®le de fichier Excel")
            sample_data = pd.DataFrame({
                "Age": [30, 45],
                "Profession": ["M√©decin", "Enseignant"],
                "Drepanocytose": ["Non", "Oui"],
                "Opere": ["Non", "Non"],
                "Transfusion_Antecedent": ["Oui", "Non"],
                "Diabete": ["Non", "Oui"],
                "Hypertension": ["Oui", "Non"],
                "Porteur_VIH_HBS_HCV": ["Non", "Non"],
                "Asthme": ["Non", "Oui"],
                "Probleme_Cardiaque": ["Non", "Non"],
                "Tatouage": ["Oui", "Non"],
                "Scarification": ["Non", "Oui"],
                "Deja_Donneur": ["Oui", "Non"],
                "Religion": ["Musulman", "Chr√©tien"],
                "Niveau_Etude":["Primaire","secondaire"],
                "Statut_Matrimonial":["Marier","Celibataire"]
            })

            # G√©n√©ration du fichier Excel √† t√©l√©charger
            output = io.BytesIO()
            with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                sample_data.to_excel(writer, index=False, sheet_name="Exemple")
                writer.close()

            st.download_button(
                label="üì• T√©l√©charger le fichier mod√®le",
                data=output.getvalue(),
                file_name="Modele_Donneurs.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )

            # ==============================
            # üì§ UPLOADER UN FICHIER EXCEL AVEC LES DONNEURS
            # ==============================
            st.subheader("üì§ Uploader un fichier Excel contenant les informations des donneurs")
            uploaded_file = st.file_uploader("T√©l√©chargez un fichier Excel", type=["xlsx"])

            if uploaded_file:
                # Lecture du fichier
                df_uploaded = pd.read_excel(uploaded_file)

                # V√©rification des colonnes attendues
                expected_columns = sample_data.columns.tolist()
                if not all(col in df_uploaded.columns for col in expected_columns):
                    st.error("‚ö† Le fichier ne contient pas les bonnes colonnes ! V√©rifiez le format et r√©essayez.")
                else:
                    st.success("‚úÖ Fichier charg√© avec succ√®s !")

                    # ==============================
                    # üîÑ PR√âTRAITEMENT DES DONN√âES POUR TOUTES LES LIGNES
                    # ==============================
                    # Supprimer les lignes avec des valeurs non reconnues
                    df_uploaded.dropna(inplace=True)
                    colonne=["Age", "Profession", "Drepanocytose","Opere",
                "Transfusion_Antecedent","Diabete", "Hypertension", "Porteur_VIH_HBS_HCV", 
                "Asthme", "Probleme_Cardiaque","Tatouage","Scarification", "Deja_Donneur", "Religion","Niveau_Etude","Statut_Matrimonial"]
                    input_data =preprocessor.transform(df_uploaded)
                    df_uploadeds=input_data.copy()
                    

                    # Encoder les valeurs binaires (Oui = 1, Non = 0)
                    if df_uploadeds.shape[0] == 0:
                        st.error("Aucune donn√©e valide apr√®s pr√©traitement ! v√©rifiez les donn√©es d'entr√©e")

                    # ==============================
                    # üöÄ PR√âDICTION SUR TOUTES LES LIGNES
                    # ==============================
                    df_uploaded["Pr√©diction"] = model.predict(df_uploadeds)

                    # D√©codage des r√©sultats
                    df_uploaded["Pr√©diction"] = df_uploaded["Pr√©diction"].map(lambda x: target_encoder.inverse_transform([x])[0])

                    # Affichage du tableau avec les pr√©dictions
                    st.subheader("üìä R√©sultats des pr√©dictions")
                    st.dataframe(df_uploaded)

                    # ==============================
                    # üì§ T√âL√âCHARGER LE FICHIER AVEC PREDICTIONS
                    # ==============================
                    output_predictions = io.BytesIO()
                    with pd.ExcelWriter(output_predictions, engine='xlsxwriter') as writer:
                        df_uploaded.to_excel(writer, index=False, sheet_name="Pr√©dictions")
                        writer.close()

                    st.download_button(
                        label="üì• T√©l√©charger les r√©sultats",
                        data=output_predictions.getvalue(),
                        file_name="Resultats_Predictions.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )

    # ==============================
    # üìà ONGLET : PERFORMANCE DU MOD√àLE
    # ==============================
        with tab3:
            # ==============================
            # üéØ CHARGEMENT DU MOD√àLE & DONN√âES TEST
            # ==============================
            @st.cache_resource
            def load_model():
               pathse="eligibility_model.pkl"
               data = joblib.load(pathse)
               return data["model"], data["X_test"], data["y_test"], data["target_encoder"], data["lpreprocessor"],data["resultat"]

            model, X_test, y_test, target_encoder, preprocessor,resultat = load_model()
                # ==============================
            # üìä PERFORMANCE DU MOD√àLE SUR DONN√âES TEST
            # ==============================
            st.subheader("üìà Performance du Mod√®le sur Donn√©es de Test")

            # üîÆ Pr√©dictions sur X_test
            y_pred_proba = model.predict_proba(X_test)[:, 1]
            y_pred = model.predict(X_test)

            # üìÑ Rapport de Classification
            st.subheader("üìÑ Rapport de Classification")
            A=target_encoder.inverse_transform([0,1,2])
            report = classification_report(y_test, y_pred, target_names=A, output_dict=True)
            df_report = pd.DataFrame(report).transpose()
            st.dataframe(df_report)
            st.dataframe(resultat)
            

            # üîÑ Binarisation des √©tiquettes pour "One vs Rest" si multi-classe
            n_classes = len(np.unique(y_test))  # Nombre de classes
            y_test_bin = label_binarize(y_test, classes=np.arange(n_classes))  # Transforme y_test en binaire
            y_pred_proba = model.predict_proba(X_test)  # Probabilit√©s pr√©dites pour chaque classe

            # üìà Affichage de la courbe ROC pour chaque classe
            st.subheader("üìà Courbe ROC (One vs Rest)")

            fig, ax = plt.subplots(figsize=(7, 5))
            colors = ['blue', 'red', 'green', 'purple', 'orange']  # Couleurs pour chaque classe

            for i in range(n_classes):
                if n_classes > 2:  # Cas multi-classe
                    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])
                    roc_auc = auc(fpr, tpr)
                    ax.plot(fpr, tpr, color=colors[i % len(colors)], lw=2, label=f"Classe {A[i]} (AUC = {roc_auc:.2f})")
                else:  # Cas binaire classique
                    fpr, tpr, _ = roc_curve(y_test, y_pred_proba[:, 1])
                    roc_auc = auc(fpr, tpr)
                    ax.plot(fpr, tpr, color="blue", lw=2, label=f"ROC curve (AUC = {roc_auc:.2f})")

            ax.plot([0, 1], [0, 1], color="gray", linestyle="--")
            ax.set_xlabel("Taux de Faux Positifs (FPR)")
            ax.set_ylabel("Taux de Vrais Positifs (TPR)")
            ax.set_title("Courbe ROC par Classe")
            ax.legend(loc="lower right")
            st.pyplot(fig)

            
            # üìä Matrice de Confusion
            st.subheader("üìä Matrice de Confusion")
            B=target_encoder.inverse_transform(y_test)
            C=target_encoder.inverse_transform(y_pred)
            conf_matrix = confusion_matrix(B, C)
            fig, ax = plt.subplots()
            sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", ax=ax)
            ax.set_title("Matrice de Confusion")
            st.pyplot(fig)
            # Afficher les probabilit√©s
            st.write("**Probabilit√©s:**")
            
            # Cr√©er un DataFrame pour les probabilit√©s
            proba_df = pd.DataFrame({
                'Statut': model.classes_,
                'Probabilit√©': y_pred_proba[0]
            })
            
            # Cr√©er un graphique √† barres pour les probabilit√©s
            fig = px.bar(
                proba_df,
                x='Statut',
                y='Probabilit√©',
                title="Probabilit√©s pour chaque statut d'√©ligibilit√©",
                labels={'Probabilit√©': 'Probabilit√©', 'Statut': "Statut d'√©ligibilit√©"}
            )
            
            fig.update_layout(
                xaxis_title="Statut d'√©ligibilit√©",
                yaxis_title="Probabilit√©",
                font=dict(size=12),
                height=400
            )
            
            st.plotly_chart(fig, use_container_width=True)

            #if hasattr(model, 'feature_importances_'):
            # Obtenir les noms des caract√©ristiques apr√®s one-hot encoding
            feature_names = []
            for name, transformer, features in preprocessor.transformers_:
                if name == 'cat':
                    # Pour les caract√©ristiques cat√©gorielles, obtenir les noms apr√®s one-hot encoding
                    for i, feature in enumerate(features):
                        categories = transformer.named_steps['onehot'].categories_[i]
                        for category in categories:
                            feature_names.append(f"{feature}_{category}")
                else:
                    # Pour les caract√©ristiques num√©riques, conserver les noms d'origine
                    feature_names.extend(features)
            
            # Obtenir les importances des caract√©ristiques
            importances = model.feature_importances_
            
            # Cr√©er un DataFrame pour les importances
            if len(feature_names) == len(importances):
                importance_df = pd.DataFrame({
                    'Feature': feature_names,
                    'Importance': importances
                })
                
                # Trier par importance d√©croissante
                importance_df = importance_df.sort_values('Importance', ascending=False).head(15)
                
                # Cr√©er un graphique des importances
                fig_importance = px.bar(
                    importance_df,
                    x='Importance',
                    y='Feature',
                    orientation='h',
                    title="Importance des caract√©ristiques pour la pr√©diction d'√©ligibilit√©",
                    labels={'Importance': 'Importance relative', 'Feature': 'Caract√©ristique'}
                )
                
                fig_importance.update_layout(
                    xaxis_title="Importance relative",
                    yaxis_title="Caract√©ristique",
                    font=dict(size=12),
                    height=600
                )
            st.plotly_chart(fig_importance)
           
                    
                    
    
    # Pied de page
    st.markdown("---")
    st.markdown(""" <div style="text-align: center;">
        <p>Tableau de bord d√©velopp√© pour le concours de data visualisation sur les donneurs de sang</p>
        <p>¬© 2025 - Tous droits r√©serv√©s</p>
    </div>
    """, unsafe_allow_html=True)

# Point d'entr√©e principal
if __name__ == "__main__":
    main()
